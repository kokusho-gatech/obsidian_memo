#!/usr/bin/env python3
"""
ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆèª¬æ˜æ›¸ä½œæˆã®ãŸã‚ã®æƒ…å ±åé›†è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
GitHub PRã€Confluenceã€ãã®ä»–ã®æƒ…å ±æºã‹ã‚‰åŒ…æ‹¬çš„ã«ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¾ã™ã€‚
"""

import os
import csv
import json
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Any
import argparse


class ProductDocumentationCollector:
    def __init__(self, project_path: str):
        self.project_path = project_path
        self.log_file = os.path.expanduser("~/.cursor/auto_execution_history.csv")
        self.output_dir = os.path.join(project_path, "collected_data")
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
        if not os.path.exists(self.log_file):
            with open(self.log_file, 'w', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['Date', 'Time', 'Command', 'Project_Path', 'Scope', 'Command_Executed', 'Content', 'Result'])
    
    def log_execution(self, command: str, scope: str, command_executed: str, content: str, result: str):
        """å®Ÿè¡Œãƒ­ã‚°ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«è¨˜éŒ²"""
        now = datetime.now()
        date_str = now.strftime("%Y/%m/%d")
        time_str = now.strftime("%H:%M")
        
        with open(self.log_file, 'a', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow([date_str, time_str, command, self.project_path, scope, command_executed, content, result])
    
    def collect_github_data(self, repo_url: str = None, days_back: int = 30) -> Dict[str, Any]:
        """GitHubã‹ã‚‰PRãƒ»ã‚³ãƒŸãƒƒãƒˆæƒ…å ±ã‚’åé›†"""
        print("ğŸ” GitHubæƒ…å ±ã‚’åé›†ä¸­...")
        
        # ã“ã“ã§ã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ GitHub API ã‚’ä½¿ç”¨
        github_data = {
            "recent_prs": [
                {
                    "number": 123,
                    "title": "æŸ»å®šè‡ªå‹•åŒ–æ©Ÿèƒ½ã®åŸºç›¤å®Ÿè£…",
                    "author": "dev-team",
                    "merged_at": "2025-01-20",
                    "description": "æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸæŸ»å®šè‡ªå‹•åŒ–ã®åŸºç›¤ã‚’å®Ÿè£…",
                    "files_changed": ["app/models/assessment.rb", "app/services/ai_assessment_service.rb"],
                    "labels": ["enhancement", "ai-feature"]
                },
                {
                    "number": 122,
                    "title": "ä»²ä»‹ãƒã‚¹ã‚¿ã®ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š",
                    "author": "data-team", 
                    "merged_at": "2025-01-15",
                    "description": "ç©ºç™½ç‡ã®é«˜ã„ä»²ä»‹ãƒã‚¹ã‚¿ãƒ‡ãƒ¼ã‚¿ã®å“è³ªå‘ä¸Šå¯¾å¿œ",
                    "files_changed": ["app/models/intermediary_company.rb", "db/migrate/"],
                    "labels": ["data-quality", "bug-fix"]
                }
            ],
            "architecture_changes": [
                {
                    "date": "2025-01-10",
                    "type": "database",
                    "description": "JSONBå‹ã«ã‚ˆã‚‹ã‚¹ã‚­ãƒ¼ãƒãƒ¬ã‚¹è¨­è¨ˆå°å…¥",
                    "impact": "æ›¸é¡ç®¡ç†ã®æŸ”è»Ÿæ€§å‘ä¸Š"
                }
            ]
        }
        
        self.log_execution(
            "GitHubæƒ…å ±åé›†",
            "GitHub Repository",
            "collect_github_data()",
            f"éå»{days_back}æ—¥é–“ã®PRãƒ»ã‚³ãƒŸãƒƒãƒˆæƒ…å ±",
            f"PR {len(github_data['recent_prs'])}ä»¶ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å¤‰æ›´ {len(github_data['architecture_changes'])}ä»¶ã‚’åé›†"
        )
        
        return github_data
    
    def collect_confluence_data(self, space: str = "SUPPLIER") -> Dict[str, Any]:
        """Confluenceã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ»è¦ä»¶æƒ…å ±ã‚’åé›†"""
        print("ğŸ“š Confluenceæƒ…å ±ã‚’åé›†ä¸­...")
        
        # ã“ã“ã§ã¯ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã—ã¾ã™
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ Confluence API ã¾ãŸã¯ MCP ã‚’ä½¿ç”¨
        confluence_data = {
            "projects": [
                {
                    "title": "SUP 2025 CRMæ©Ÿèƒ½æ§‹ç¯‰ph.1",
                    "status": "é€²è¡Œä¸­",
                    "background": "ä¸­æœŸçµŒå–¶è¨ˆç”»ã«å‘ã‘ãŸäº‹æ¥­ã‚¹ã‚±ãƒ¼ãƒ«ã§10,000ç¤¾å¯¾å¿œãŒå¿…è¦",
                    "objectives": ["é¡§å®¢ç®¡ç†æ©Ÿèƒ½ä½œæˆ", "ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š"],
                    "last_updated": "2025-01-28"
                },
                {
                    "title": "SUP 2025 ä»²ä»‹ãƒã‚¹ã‚¿æ”¹ä¿®",
                    "status": "å®Œäº†",
                    "background": "ä»²ä»‹ãƒã‚¹ã‚¿ã®ç©ºç™½ç‡ãŒé«˜ã„ï¼ˆå¸‚åŒºç”ºæ‘ï¼š91.6%ã€æ”¯åº—åï¼š95.1%ï¼‰",
                    "objectives": ["ãƒ‡ãƒ¼ã‚¿å“è³ªå‘ä¸Š", "æ¤œç´¢æ©Ÿèƒ½æ”¹å–„"],
                    "last_updated": "2025-01-23"
                }
            ],
            "technical_decisions": [
                {
                    "title": "æŸ”è»Ÿæ€§é‡è¦–ã®DBè¨­è¨ˆã«ã¤ã„ã¦",
                    "decision": "JSONBå‹ã«ã‚ˆã‚‹ã‚¹ã‚­ãƒ¼ãƒãƒ¬ã‚¹è¨­è¨ˆæ¡ç”¨",
                    "rationale": "æ›¸é¡ç¨®åˆ¥ãƒ»é …ç›®ã®é »ç¹ãªè¿½åŠ ã«å¯¾å¿œã™ã‚‹ãŸã‚",
                    "trade_offs": "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ vs æŸ”è»Ÿæ€§"
                }
            ]
        }
        
        self.log_execution(
            "Confluenceæƒ…å ±åé›†",
            "Confluence Space",
            "collect_confluence_data()",
            f"ã‚¹ãƒšãƒ¼ã‚¹: {space}ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ»æŠ€è¡“æƒ…å ±",
            f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ {len(confluence_data['projects'])}ä»¶ã€æŠ€è¡“æ±ºå®š {len(confluence_data['technical_decisions'])}ä»¶ã‚’åé›†"
        )
        
        return confluence_data
    
    def analyze_user_teams(self) -> Dict[str, Any]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ»ãƒãƒ¼ãƒ åˆ†æ"""
        print("ğŸ‘¥ ãƒ¦ãƒ¼ã‚¶ãƒ»ãƒãƒ¼ãƒ æƒ…å ±ã‚’åˆ†æä¸­...")
        
        user_teams_data = {
            "primary_users": [
                {
                    "role": "æŸ»å®šæ‹…å½“è€…",
                    "team": "ä¸å‹•ç”£è©•ä¾¡ãƒãƒ¼ãƒ ",
                    "main_features": ["æŸ»å®šæ©Ÿèƒ½", "ä¾¡æ ¼äºˆæ¸¬", "ç‰©ä»¶æƒ…å ±ç®¡ç†"],
                    "pain_points": ["æŸ»å®šæ™‚é–“ã®é•·ã•", "ãƒ‡ãƒ¼ã‚¿å…¥åŠ›ã®æ‰‹é–“"],
                    "success_metrics": ["æŸ»å®šç²¾åº¦", "å‡¦ç†æ™‚é–“çŸ­ç¸®"]
                },
                {
                    "role": "ä»²ä»‹ç®¡ç†è€…", 
                    "team": "ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ç®¡ç†ãƒãƒ¼ãƒ ",
                    "main_features": ["ä»²ä»‹ä¼šç¤¾ç®¡ç†", "ã‚¹ã‚¿ãƒƒãƒ•ç®¡ç†", "å–å¼•å±¥æ­´"],
                    "pain_points": ["ãƒ‡ãƒ¼ã‚¿å“è³ªã®ä½ã•", "æ¤œç´¢æ€§ã®æ‚ªã•"],
                    "success_metrics": ["ãƒ‡ãƒ¼ã‚¿å®Œæˆåº¦", "æ¤œç´¢åŠ¹ç‡"]
                }
            ],
            "stakeholders": [
                {
                    "role": "ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼",
                    "interests": ["æ©Ÿèƒ½ã®å„ªå…ˆé †ä½æ±ºå®š", "ãƒ¦ãƒ¼ã‚¶ä¾¡å€¤æœ€å¤§åŒ–"],
                    "involvement": "è¦ä»¶å®šç¾©ã€å„ªå…ˆé †ä½æ±ºå®š"
                },
                {
                    "role": "ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼",
                    "interests": ["æŠ€è¡“çš„å®Ÿç¾æ€§", "ä¿å®ˆæ€§"],
                    "involvement": "æŠ€è¡“æ–¹é‡æ±ºå®šã€å®Ÿè£…è¨ˆç”»"
                }
            ]
        }
        
        self.log_execution(
            "ãƒ¦ãƒ¼ã‚¶ãƒ»ãƒãƒ¼ãƒ åˆ†æ",
            "Product Analysis",
            "analyze_user_teams()",
            "ãƒ¦ãƒ¼ã‚¶å½¹å‰²ãƒ»ãƒãƒ¼ãƒ æ§‹æˆãƒ»ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼åˆ†æ",
            f"ä¸»è¦ãƒ¦ãƒ¼ã‚¶ {len(user_teams_data['primary_users'])}ç¨®ã€ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ {len(user_teams_data['stakeholders'])}ç¨®ã‚’åˆ†æ"
        )
        
        return user_teams_data
    
    def generate_architecture_overview(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦ã®ç”Ÿæˆ"""
        print("ğŸ—ï¸ ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æƒ…å ±ã‚’æ•´ç†ä¸­...")
        
        architecture_data = {
            "current_stack": {
                "backend": "Ruby on Rails",
                "database": "PostgreSQL",
                "cache": "Redis", 
                "queue": "Sidekiq",
                "storage": "AWS S3",
                "monitoring": "Datadog, Rollbar"
            },
            "evolution": [
                {
                    "period": "2024å¹´",
                    "changes": ["åŸºæœ¬æ©Ÿèƒ½å®Ÿè£…", "èªè¨¼ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰"],
                    "technologies_added": ["devise", "omniauth-google-oauth2"]
                },
                {
                    "period": "2025å¹´Q1",
                    "changes": ["ã‚¹ã‚­ãƒ¼ãƒãƒ¬ã‚¹è¨­è¨ˆå°å…¥", "CRMæ©Ÿèƒ½åŸºç›¤"],
                    "technologies_added": ["JSONBæ´»ç”¨", "Confluenceé€£æº"]
                }
            ],
            "future_roadmap": [
                {
                    "timeline": "2025å¹´Q2",
                    "planned_changes": ["ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹åŒ–æ¤œè¨", "APIé€£æºå¼·åŒ–"],
                    "drivers": ["ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£è¦æ±‚", "å¤–éƒ¨é€£æºå¢—åŠ "]
                }
            ]
        }
        
        self.log_execution(
            "ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£åˆ†æ",
            "System Architecture",
            "generate_architecture_overview()",
            "æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ãƒ»é€²åŒ–ãƒ»å°†æ¥è¨ˆç”»ã®æ•´ç†",
            f"ç¾åœ¨ã®æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã€é€²åŒ– {len(architecture_data['evolution'])}æ®µéšã€å°†æ¥è¨ˆç”» {len(architecture_data['future_roadmap'])}é …ç›®ã‚’æ•´ç†"
        )
        
        return architecture_data
    
    def save_collected_data(self, all_data: Dict[str, Any]):
        """åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        os.makedirs(self.output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_file = os.path.join(self.output_dir, f"product_data_{timestamp}.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(all_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ åé›†ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")
        return output_file
    
    def run_full_collection(self) -> str:
        """å®Œå…¨ãªæƒ…å ±åé›†ã‚’å®Ÿè¡Œ"""
        print("ğŸš€ ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆèª¬æ˜æ›¸ç”¨ã®æƒ…å ±åé›†ã‚’é–‹å§‹ã—ã¾ã™...")
        
        all_data = {
            "collection_timestamp": datetime.now().isoformat(),
            "github_data": self.collect_github_data(),
            "confluence_data": self.collect_confluence_data(),
            "user_teams_data": self.analyze_user_teams(),
            "architecture_data": self.generate_architecture_overview()
        }
        
        output_file = self.save_collected_data(all_data)
        
        self.log_execution(
            "å®Œå…¨æƒ…å ±åé›†",
            "Full Product Documentation",
            "run_full_collection()",
            "GitHub, Confluence, ãƒ¦ãƒ¼ã‚¶åˆ†æ, ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å…¨æƒ…å ±åé›†",
            f"ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†ã€‚å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_file}"
        )
        
        print("âœ… æƒ…å ±åé›†ãŒå®Œäº†ã—ã¾ã—ãŸï¼")
        print(f"ğŸ“Š åé›†ãƒ‡ãƒ¼ã‚¿ã®æ´»ç”¨æ–¹æ³•:")
        print(f"   1. Obsidianã§ãƒãƒ¼ãƒˆä½œæˆæ™‚ã®å‚è€ƒè³‡æ–™ã¨ã—ã¦æ´»ç”¨")
        print(f"   2. ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆèª¬æ˜æ›¸ã®æ›´æ–°ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦åˆ©ç”¨")
        print(f"   3. ãƒãƒ¼ãƒ å…±æœ‰è³‡æ–™ã¨ã—ã¦Slackç­‰ã§å…±æœ‰")
        
        return output_file


def main():
    parser = argparse.ArgumentParser(description='ãƒ—ãƒ­ãƒ€ã‚¯ãƒˆèª¬æ˜æ›¸ç”¨ã®æƒ…å ±åé›†')
    parser.add_argument('--project-path', default='/Users/t_kokusho/Documents/My%20Valut',
                       help='ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹')
    parser.add_argument('--github-repo', help='GitHubãƒªãƒã‚¸ãƒˆãƒªURL')
    parser.add_argument('--confluence-space', default='SUPPLIER', help='Confluenceã‚¹ãƒšãƒ¼ã‚¹å')
    
    args = parser.parse_args()
    
    collector = ProductDocumentationCollector(args.project_path)
    output_file = collector.run_full_collection()
    
    print(f"\nğŸ“‹ å®Ÿè¡Œãƒ­ã‚°: {collector.log_file}")
    print(f"ğŸ“ åé›†ãƒ‡ãƒ¼ã‚¿: {output_file}")


if __name__ == "__main__":
    main() 